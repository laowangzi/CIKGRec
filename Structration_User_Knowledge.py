# Read the file generated by chatGPT and apply interest clustering (merge similar interests)
import os
import time
import re
import jsonlines
import numpy as np
import pandas as pd
from tqdm import tqdm
from utils import clean_text
#Packages for Similarity Calculation
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer

dataset = 'book-crossing'
if dataset in ['dbbook2014', 'book-crossing']:
    field = 'books'
else:
    field = 'movies'
    
#Select in '' or '_r'. Two text cleaning strategy.
r = '_r'
C = 350
max_his_num = 30

#Select in [st, tfidf]; 'st' means 'SentenceTransformer'
cluster_type = 'tfidf'
data_path = f'./data/{dataset}'
#Replace your file name
output_path = f'batch_output/{dataset}_max{max_his_num}_output.jsonl'

if dataset in ['dbbook2014', 'ml1m']:
    item_list = pd.read_csv(data_path+'/item_list.txt', sep=' ')
    entity_list = pd.read_csv(data_path+'/entity_list.txt', sep=' ')
else:
    item_list = list()    
    entity_list = list()
    lines = open(data_path+'/item_list.txt', 'r').readlines()
    for l in lines[1:]:
        if l == "\n":
            continue
        tmps = l.replace("\n", "").strip()
        elems = tmps.split(' ')
        org_id = elems[0]
        remap_id = elems[1]
        if len(elems[2:]) == 0:
            continue
        title = ' '.join(elems[2:])    
        item_list.append([org_id, remap_id, title])
    item_list = pd.DataFrame(item_list, columns=['org_id', 'remap_id', 'entity_name'])
    lines = open(data_path+'/entity_list.txt', 'r').readlines()
    for l in lines[1:]:
        if l == "\n":
            continue
        tmps = l.replace("\n", "").strip()
        elems = tmps.split()
        org_id = elems[0]
        remap_id = elems[1]
        # entity_name = elems[2]  
        entity_list.append([org_id, remap_id])
    entity_list = pd.DataFrame(entity_list, columns=['org_id', 'remap_id'])
    

kg = pd.read_csv(data_path+'/kg_final.txt', sep=' ', names=['head', 'relation', 'tail'])
relation = pd.read_csv(data_path+'/relation_list.txt', sep=' ')
entity_list['remap_id'] = entity_list['remap_id'].astype(int)

relation_intent = kg['relation'].max() + 1
relation_sim = kg['relation'].max() + 2
items = item_list['remap_id'].to_list()

#Avoid too short/long generated text (outliers)
lower = 1
upper = 50
llm_answer = []
intent_triples = []
sim_triples = set()
intents_list = []
check_intent = []
waste = []
with open(output_path, mode='r') as f:
    for answer in jsonlines.Reader(f):
        row_id = answer['custom_id']
        raw_intents = answer['response']['body']['choices'][0]['message']['content']
        clean_intents = [clean_text(it) for it in raw_intents.strip().split(',')]
        
        intents = []
        for it in clean_intents:
            if r == '':
                if len(it.split()) > lower and len(it.split()) <= upper:
                    intents.append(it)
                else:
                    print(f'waste:', it.split())
                    waste.append(it)
            elif r == '_r':
                if len(it) > lower and len(it) <= upper:
                    intents.append(it)
                else:
                    print(f'waste:', it)
                    waste.append(it)

        for it in intents:
            intent_triples.append([row_id, it])
        intents_list.extend(intents)


def encode_intents(cluster_type, intents_list):
    if cluster_type == 'st':
        #Sentence Transformer
        # Initialize the model
        device = 'cuda:0'
        #'replace into your Sentence Transformer path'
        model = SentenceTransformer('./all-MiniLM-L6-v2')
        model.to(device)
        sentences = intents_list
        embeddings = model.encode(sentences, device=device)
    elif cluster_type == 'tfidf':
        #TFIDF
        sentences = intents_list
        # Compute TF-IDF vectors
        vectorizer = TfidfVectorizer(max_df=0.8, ngram_range=(1, 2))
        embeddings = vectorizer.fit_transform(sentences)
    return embeddings


embeddings = encode_intents(cluster_type, intents_list)

print(f'cluster num: {C}')
kmeans = KMeans(n_clusters=C, n_init='auto')  # You can adjust the number of clusters as needed
cluster_ids = kmeans.fit_predict(embeddings)
# Create the output dictionary
output_dict = dict(zip(intents_list, cluster_ids))

#Decide the generated interet ID
entity_add = []
entity_max = entity_list['remap_id'].max()+1
for k, v in output_dict.items():
    entity_add.append([k, v+entity_max])
    output_dict[k] = v + entity_max
entity_add = pd.DataFrame(entity_add, columns=['org_id', 'remap_id'])

intent_triples_df = pd.DataFrame(intent_triples, columns=['uid', 'interest'])
intent_triples_df['merged_interest'] = intent_triples_df['interest'].map(output_dict)
user_num = len(intent_triples_df['uid'].unique())
print(f'user num:{user_num}')

intent_triples_df_group = intent_triples_df.groupby('merged_interest').count()
intent_triples_df_sort = intent_triples_df_group.sort_values(by='uid', ascending=False)
#Delet the interest with too much users
del_list = []
del_list.extend(list(intent_triples_df_sort[intent_triples_df_sort['uid']>=int(user_num/5)].index))
print(len(del_list))
del_list.extend(intent_triples_df_sort[intent_triples_df_sort['uid']==1].index)
intent_triples_df_filter = intent_triples_df[~intent_triples_df['merged_interest'].isin(del_list)]
print('interest graph edge num:', len(intent_triples_df_filter))
print('interest graph sparsity:', round(len(intent_triples_df_filter)/(user_num*C), 4)*100, '%')
kg_intent = intent_triples_df_filter.loc[:, ['uid', 'merged_interest']]
kg_intent.columns = ['uid', 'interest']
#We provide our processed file named: user_interest_clustered.txt
kg_intent.to_csv(data_path+f'/kg_interest_C{C}_{cluster_type}_{version}{r}.txt', sep=' ', index=False, header=None)